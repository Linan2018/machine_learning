# -*- coding: utf-8 -*-
"""ch_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QHkeYB3K0t6nnHNEQ_qbzRML7jMBKJpO
"""

cd drive/My\ Drive/ML_hw

pwd

import numpy as np
import copy
import random
import matplotlib.pyplot as plt
import pandas as pd

np.random.seed(0)

!ls

df = pd.read_csv('iris.data')

df

df = df.replace('Iris-setosa', 0)
df = df.replace('Iris-versicolor', 1)
df = df.replace('Iris-virginica', 2)

df

from sklearn import datasets
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier

def gen_data(x, y, tar):
  n0 = np.sum(y == 0)
  n1 = np.sum(y == 1)
  n2 = np.sum(y == 2)

  y_ = y.copy()
  if tar != -1:
    index1 = np.where(y == tar)
    y_[index1] = 1
    index2 = np.where(y != tar)
    y_[index2] = 0

  # 随机划分训练集和测试集
  num = x.shape[0]
  ratio = 7/3
  num_test = int(num/(1+ratio))
  num_train = num - num_test

  # 打乱并划分
  index = np.arange(num)
  np.random.shuffle(index)
  x_test = x[index[:num_test],:]
  y_test = y_[index[:num_test]]
  x_train = x[index[num_test:],:]
  y_train = y_[index[num_test:]]

  return x_train, y_train, x_test, y_test

iris = datasets.load_iris() 
print(type(iris), dir(iris))

x = iris.get('data')
y = iris.get('target')

x_train0, y_train0, x_test0, y_test0 = gen_data(x, y, 0)
x_train1, y_train1, x_test1, y_test1 = gen_data(x, y, 1)
x_train2, y_train2, x_test2, y_test2 = gen_data(x, y, 2)

clf_linear0 = svm.SVC()
clf_linear0.fit(x_train0, y_train0)

y_test_pre_linear0 = clf_linear0.predict(x_test0)

# 计算准确率
acc_linear0 = sum(y_test_pre_linear0==y_test0)/len(y_test0)
print('class 0 against rest: The accuracy0 is', acc_linear0) 

clf_linear1 = svm.SVC()
clf_linear1.fit(x_train1, y_train1)

y_test_pre_linear1 = clf_linear1.predict(x_test1)

# 计算准确率
acc_linear1 = sum(y_test_pre_linear1==y_test1)/len(y_test1)
print('class 1 against rest: The accuracy1 is', acc_linear1) 

clf_linear2 = svm.SVC()
clf_linear2.fit(x_train2, y_train2)

y_test_pre_linear2 = clf_linear2.predict(x_test2)

# 计算准确率
acc_linear2 = sum(y_test_pre_linear2==y_test2)/len(y_test2)
print('class 2 against rest: The accuracy2 is', acc_linear2)

i = np.where(y == 2)
i



x_train, y_train, x_test, y_test = gen_data(x, y, -1)

knn = KNeighborsClassifier() 
knn.fit(x_train, y_train)

iris_y_predict = knn.predict(x_test) 
probility=knn.predict_proba(x_test)
score=knn.score(x_test,y_test,sample_weight=None)

print('y_predict = ',iris_y_predict)
print('y_test = ', y_test)
print('The accuracy is:' ,score)

y_test

import keras
from keras.models import Sequential
from keras.layers import Dense,Activation
from keras.losses import categorical_crossentropy
from keras.optimizers import SGD,Adam

x_train0

# x_train0, y_train0, x_test0, y_test0
e = 200

y_train0 = y_train0.reshape(len(y_train0),1)
y_test0 = y_test0.reshape(len(y_test0),1)

y_train1 = y_train1.reshape(len(y_train1),1)
y_test1 = y_test1.reshape(len(y_test1),1)

y_train2 = y_train2.reshape(len(y_train2),1)
y_test2 = y_test2.reshape(len(y_test2),1)

model0 = Sequential()

model0.add(Dense(10, input_shape=((4,)), activation='sigmoid'))
model0.add(Dense(10, activation='sigmoid'))
model0.add(Dense(1, activation='sigmoid'))

sgd = SGD(lr=0.01, decay=1e-6)
model0.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])

history0 = model0.fit(x_train0, y_train0, validation_data=(x_test0, y_test0), epochs=e, batch_size=8)

epochs0 = range(len(history0.history['accuracy']))

plt.plot(epochs0,history0.history['accuracy'],'b',label='Training acc')
plt.plot(epochs0,history0.history['val_accuracy'],'r',label='Validation acc')
plt.title('Traing and Validation accuracy')

model1 = Sequential()

model1.add(Dense(10, input_shape=((4,)), activation='sigmoid'))
model1.add(Dense(10, activation='sigmoid'))
model1.add(Dense(1, activation='sigmoid'))

sgd = SGD(lr=0.01, decay=1e-6)
model1.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])

history1 = model1.fit(x_train1, y_train1, validation_data=(x_test1, y_test1), epochs=e, batch_size=8)

epochs1 = range(len(history1.history['accuracy']))

plt.plot(epochs1,history1.history['accuracy'],'b',label='Training acc')
plt.plot(epochs1,history1.history['val_accuracy'],'r',label='Validation acc')
plt.title('Traing and Validation accuracy')

model2 = Sequential()

model2.add(Dense(10, input_shape=((4,)), activation='sigmoid'))
model2.add(Dense(10, activation='sigmoid'))
model2.add(Dense(1, activation='sigmoid'))

sgd = SGD(lr=0.01, decay=1e-6)
model2.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])

history2 = model0.fit(x_train2, y_train2, validation_data=(x_test2, y_test2), epochs=200, batch_size=8)

epochs2 = range(len(history2.history['accuracy']))

plt.plot(epochs2,history2.history['accuracy'],'b',label='Training acc')
plt.plot(epochs2,history2.history['val_accuracy'],'r',label='Validation acc')
plt.title('Traing and Validation accuracy')

fig = plt.figure(figsize=(9, 12))
# ax = fig.add_subplot(111)
# ax.set_aspect('equal', adjustable='box')


plt.subplot(321)
plt.plot(epochs0,history0.history['loss'],'b',label='Training loss')
plt.plot(epochs0,history0.history['val_loss'],'r',label='Validation val_loss')
plt.title('Traing and Validation loss (0 against rest)')
plt.legend()
plt.tight_layout()
# ax = plt.gca()
# ax.set_aspect(1)

plt.subplot(322)
plt.plot(epochs0,history0.history['accuracy'],'b',label='Training acc')
plt.plot(epochs0,history0.history['val_accuracy'],'r',label='Validation acc')
plt.title('Traing and Validation accuracy (0 against rest)')
plt.legend()
plt.tight_layout()

plt.subplot(323)
plt.plot(epochs1,history1.history['loss'],'b',label='Training loss')
plt.plot(epochs1,history1.history['val_loss'],'r',label='Validation val_loss')
plt.title('Traing and Validation loss (1 against rest)')
plt.legend()
plt.tight_layout()
# ax = plt.gca()
# ax.set_aspect(1)

plt.subplot(324)
plt.plot(epochs1,history1.history['accuracy'],'b',label='Training acc')
plt.plot(epochs1,history1.history['val_accuracy'],'r',label='Validation acc')
plt.title('Traing and Validation accuracy (1 against rest)')
plt.legend()
plt.tight_layout()

plt.subplot(325)
plt.plot(epochs2,history2.history['loss'],'b',label='Training loss')
plt.plot(epochs2,history2.history['val_loss'],'r',label='Validation val_loss')
plt.title('Traing and Validation loss (2 against rest)')
plt.legend()
plt.tight_layout()
# ax = plt.gca()
# ax.set_aspect(1)

plt.subplot(326)
plt.plot(epochs2,history2.history['accuracy'],'b',label='Training acc')
plt.plot(epochs2,history2.history['val_accuracy'],'r',label='Validation acc')
plt.title('Traing and Validation accuracy (2 against rest)')
plt.legend()
plt.tight_layout()

plt.savefig('6_result.svg')

x = np.asarray([[.697,.460],
 [.774,.376],
 [.634,.264],
 [.608,.318],
 [.556,.215],
 [.403,.237],
  [.481,.149],
  [.437,.211],
  [.666,.091],
  [.243,.267],
  [.245,.057],
  [.343,.099],
  [.639,.161],
  [.657,.198],
  [.360,.370],
  [.593,.042],
 [.719,.103]
 ])

def draw(x, y, m='.'):
  p = []
  n = []
  for i in range(len(x)):
    if y[i] > 0.5:
      p.append(x[i])
    else:
      n.append(x[i])
  p = np.asarray(p)
  n = np.asarray(n)
  if p.size > 0:
    plt.scatter(p[:, 0], p[:, 1], c='b', marker=m)
  if n.size > 0:
    plt.scatter(n[:, 0], n[:, 1], c='r', marker=m)

def draw_v(x, m='.'):
    plt.scatter(n[:, 0], n[:, 1], c='r', marker=m)

y = np.asarray([1]*8+[0]*9)

draw(x,y)

clf_linear = svm.SVC(kernel="linear")
clf_rbf = svm.SVC(kernel="rbf")
clf_linear.fit(x, y)
clf_rbf.fit(x, y)

y_test_pre_linear = clf_linear.predict(x)
y_test_pre_rbf = clf_rbf.predict(x)


# 计算分类准确率
acc_linear = sum(y_test_pre_linear==y)/len(y)
print('linear kernel: The accuracy is', acc_linear) 
acc_rbf = sum(y_test_pre_rbf==y)/len(y)
print('rbf kernel: The accuracy is', acc_rbf)

dir(clf_linear)



v1 = clf_linear.support_vectors_
v2 = clf_rbf.support_vectors_


fig = plt.figure(figsize=(8, 4))
# ax = fig.add_subplot(111)
# ax.set_aspect('equal', adjustable='box')


plt.subplot(121)
plt.title('Linear kernel')
draw(x,y)
plt.scatter(x[y_test_pre_linear==y][:, 0], x[y_test_pre_linear==y][:, 1], s=100, c = '', alpha=0.5, linewidth=1.5, edgecolor='g')
plt.tight_layout()
# ax = plt.gca()
# ax.set_aspect(1)

plt.subplot(122)
plt.title('Gaussian kernel')
draw(x,y)
plt.scatter(x[y_test_pre_rbf==y][:, 0], x[y_test_pre_rbf==y][:, 1], s=100, c = '', alpha=0.5, linewidth=1.5, edgecolor='g')
plt.tight_layout()
# ax = plt.gca()
# ax.set_aspect(1)
plt.savefig('6_svm.svg')

draw(x,y)

